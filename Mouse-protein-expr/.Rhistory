# make predictions using valid set
raw.pred <- data.frame(neuralnet::compute(model,
data.frame(data[,-ncol(data)]))$net.result)
# initialize target labels
labels <- c("c-CS-m", "c-CS-s", "c-SC-m", "c-SC-s",
"t-CS-m", "t-CS-s", "t-SC-m", "t-SC-s")
pred.label <- data.frame(max.col(raw.pred)) %>%
mutate(prediction=labels[max.col.raw.pred.])
# make predictions using valid set
preds <- as.factor(pred.label[,2])
confMat <- confusionMatrix(validX$Class, preds)
confMat
}
NN.cm.valid <- tablePred.NN(NN.model, validX)
NN.cm.valid
### multinomial logistic regression ###
set.seed(232)
# fit the model
MGR.model <- nnet::multinom(formula.NC, data = trainX, trace = F)
# make predictions using valid set
MGR.pred <- predict(MGR.model, newdata=validX)
# check reference and predicted classification
MGR.cm.valid <- confusionMatrix(validX$Class, MGR.pred)
MGR.cm.valid
# Custom Stacked Ensemble learner
stackLearner <- function(model1, model2, model3, testData, validData){
## --NB.model: model1 (Naive Bayes)
## --NN.model: model2 (Artificial Neural Network)
## --MGR.model: model3 (Multinomial Logistic regression)
# raw predictions from the base models
validpred1 <- predict(model1, validData, type= "raw")
validpred2 <- predict(model2, validData, type= "raw")
colnames(validpred2) <- colnames(validpred1)
validpred3 <- predict(model3, validData, type= "probs")
# Generate level-one dataset for training the ensemble meta-learner
basePred <- data.frame(validpred1, validpred2, validpred3, Class= validData$Class,
stringsAsFactors = F)
# Train the ensemble
set.seed(270)
modelStack <- train(Class~., data = basePred, method = "rf")
# Generate predictions on the test set
testPred1 <- predict(model1, newdata = testData,type= "raw")
testPred2 <- predict(model2, newdata = testData, type= "raw")
testPred3 <- predict(model3, newdata = testData,type= "probs")
# Using the base learner test set predictions,
# create the level-one dataset to feed to the ensemble
testPredLevelOne <- data.frame(testPred1, testPred2, testPred3,
Class= testX$Class, stringsAsFactors = F)
# estimate predictions over tested base models
colnames(testPredLevelOne) <- colnames(basePred)
combPred <- predict(modelStack, testPredLevelOne)
# Evaluate ensemble test performance
confMatrix <- confusionMatrix(combPred, testData$Class)
return(list(confMatrix, combPred))
}
# func call
stackResults  <- stackLearner(model1 = NB.model,
model2 = NN.model,
model3 = MGR.model,
testData = testX, validData = validX)
# Ensemble prediction matrix
ENS.confMat <- unlist(stackResults[1])
ENS.confMat
# Ensemble prediction matrix
ENS.confMat <- stackResults[1]
ENS.confMat
# Evaluate base learners test performances
#model1
NB.pred.test <- predict(NB.model, testX)
NB.cm.test <- confusionMatrix(NB.pred.test, testX$Class, mode='prec_recall')
NB.cm.test$table
# model2
NN.cm.test <- tablePred.NN(NN.model, testX)
NN.cm.test
# model3
MGR.pred.test <- predict(MGR.model, testX)
MGR.cm.test <- confusionMatrix(MGR.pred.test, testX$Class)
MGR.cm.test
## Plotting ROC curve of our best model --stacked ensemble learner
stack.ens.pred <- unlist(stackResults[2])
roc.pred <- as.ordered(stack.ens.pred)
# get aucs for every class label
result <- pROC::multiclass.roc(testX$Class, roc.pred, quiet = T,
levels = c("c-CS-m", "c-CS-s", "c-SC-m", "c-SC-s",
"t-CS-m", "t-CS-s", "t-SC-m", "t-SC-s"))
# plotting the classes that are significant to our learning outcomes
# normal learning: "c-CS-m"
rocModels <- function(pred, test){
roc.pred <- as.ordered(pred)
# get roc results for every class label
res<- pROC::multiclass.roc(test$Class, roc.pred, quiet = T,
levels = c("c-CS-m", "c-CS-s", "c-SC-m", "c-SC-s",
"t-CS-m", "t-CS-s", "t-SC-m", "t-SC-s"))
plot.roc(res$rocs[[1]],
print.auc=T,
legacy.axes = T)
# normal learning: "c-CS-s"
plot.roc(res$rocs[[2]],
add=T, col = 'red',
print.auc = T,
legacy.axes = T,
print.auc.adj = c(0,3))
# rescued learning: "t-CS-m"
plot.roc(res$rocs[[4]],add=T, col = 'blue',
print.auc=T,
legacy.axes = T,
print.auc.adj = c(0,5))
# failed learning: "t-CS-s"
plot.roc(res$rocs[[5]],
add=T, col = 'green',
print.auc = T,
legacy.axes = T,
print.auc.adj = c(0,7))
legend('bottomright',
legend = c('c-CS-m: NL-1','c-CS-s: NL-2',
't-CS-m: RL', 't-CS-s: FL'),
col=c('black','red','blue', 'green'),lwd=2)
}
rocModels(pred=stack.ens.pred, test = testX)
# plotting the classes that are significant to our learning outcomes
# normal learning: "c-CS-m"
rocModels <- function(pred, test){
roc.pred <- as.ordered(pred)
# get roc results for every class label
res<- pROC::multiclass.roc(test$Class, roc.pred, quiet = T,
levels = c("c-CS-m", "c-CS-s", "c-SC-m", "c-SC-s",
"t-CS-m", "t-CS-s", "t-SC-m", "t-SC-s"))
plot.roc(res$rocs[[1]],
print.auc=T,
legacy.axes = T, main="AUC plot")
# normal learning: "c-CS-s"
plot.roc(res$rocs[[2]],
add=T, col = 'red',
print.auc = T,
legacy.axes = T,
print.auc.adj = c(0,3))
# rescued learning: "t-CS-m"
plot.roc(res$rocs[[4]],add=T, col = 'blue',
print.auc=T,
legacy.axes = T,
print.auc.adj = c(0,5))
# failed learning: "t-CS-s"
plot.roc(res$rocs[[5]],
add=T, col = 'green',
print.auc = T,
legacy.axes = T,
print.auc.adj = c(0,7))
legend('bottomright',
legend = c('c-CS-m: NL-1','c-CS-s: NL-2',
't-CS-m: RL', 't-CS-s: FL'),
col=c('black','red','blue', 'green'),lwd=2)
}
rocModels(pred=stack.ens.pred, test = testX)
# Evaluate base learners test performances
#model1
NB.pred.test <- predict(NB.model, testX)
NB.cm.test <- confusionMatrix(NB.pred.test, testX$Class, mode='prec_recall')
NB.cm.test$table
# model2
NN.cm.test <- tablePred.NN(NN.model, testX)
NN.cm.test
# model3
MGR.pred.test <- predict(MGR.model, testX)
MGR.cm.test <- confusionMatrix(MGR.pred.test, testX$Class)
MGR.cm.test
rocModels(pred=NB.pred.test, test=testX)
tablePred.NN <- function(model, data){
# make predictions using valid set
raw.pred <- data.frame(neuralnet::compute(model,
data.frame(data[,-ncol(data)]))$net.result)
# initialize target labels
labels <- c("c-CS-m", "c-CS-s", "c-SC-m", "c-SC-s",
"t-CS-m", "t-CS-s", "t-SC-m", "t-SC-s")
pred.label <- data.frame(max.col(raw.pred)) %>%
mutate(prediction=labels[max.col.raw.pred.])
# make predictions using valid set
preds <- as.factor(pred.label[,2])
confMat <- confusionMatrix(validX$Class, preds)
return(list(preds,confMat))
}
NN.cm.valid <- tablePred.NN(NN.model, validX)
NN.cm.valid <- unlist(tablePred.NN(NN.model, validX))
NN.cm.valid[2]
NN.cm.valid[1]
NN.cm.valid <- tablePred.NN(NN.model, validX)
NN.cm.valid[1]
NN.cm.valid[2]
NN.cm.test[2]
NN.cm.test[1]
# model2
NN.cm.test <- tablePred.NN(NN.model, testX)
NN.cm.test[2]
NN.cm.test[2]$table
NN.cm.test <- unlist(NN.cm.test[2])
NN.cm.test$table
NN.cm.test
NN.cm.test <- NN.cm.test[2]
NN.cm.test$table
NN.cm.test
# model2
NN.cm.test <- tablePred.NN(NN.model, testX)
NN.cm.test <- NN.cm.test[2]
NN.cm.test
NN.cm.test[1]
NN.cm.test[2]
NN.cm.test[[2]]
NN.cm.test[[1]]
unlist(NN.cm.test)
# model2
NN.tablePred <- tablePred.NN(NN.model, testX)
NN.cm.test <- NN.tablePred[2]
NN.cm.test
NN.cm.test[1,]
NN.cm.test[]
NN.cm.test
unlist(NN.cm.test[1])
NN.cm.test
# model-2: Artificial Neural Network
evalMetrics(confMat = NN.tablePred[2])
# model-2: Artificial Neural Network
evalMetrics(confMat = NN.cm.test)
# model-2: Artificial Neural Network
rocModels(pred=NN.tablePred[2], test = testX)
# model-2: Artificial Neural Network
rocModels(pred=unlist(NN.tablePred[2]), test = testX)
Q
unlist(NN.tablePred[2])
# model2
NN.tablePred <- tablePred.NN(NN.model, testX)
# model-2: Artificial Neural Network
rocModels(pred=unlist(NN.tablePred[1]), test = testX)
# model-1: Naive Bayes
rocModels(pred=NB.pred.test, test=testX)
# model-2: Artificial Neural Network
rocModels(pred=unlist(NN.tablePred[1]), test = testX)
# model-3: Multi-nomial Logistic Regression
evalMetrics(confMat = MGR.cm.test, test = testX)
(pred=MGR.pred.test.test, test = testX)
# model-3: Multi-nomial Logistic Regression
rocModels(pred=MGR.pred.test.test, test = testX)
# model-3: Multi-nomial Logistic Regression
rocModels(pred=MGR.pred.test, test = testX)
#####################################
########## Feature Study ############
#####################################
## Method-1: Correlation Cut-off
rawData <- norm_ncortex
# compute the correlation matrix
corMatNC1 <- cor(rawData)
#####################################
########## Feature Study ############
#####################################
## Method-1: Correlation Cut-off
rawData <- scaled_ncortex
# compute the correlation matrix
corMatNC1 <- cor(rawData)
# visualize the matrix, clustering features by correlation index
corrplot(corMatNC1, order = "hclust")
# After inspecting the matrix, we set the correlation threshold at 0.75
# Apply correlation filter at 0.75
highlyCor <- findCorrelation(corMatNC1, 0.75)
#then we remove all the variable correlated with more 0.75.
reduced_data <- rawData[,-highlyCor]
## Method -2: Variable Importance
feat <- reduced_data
feat$Class <- nc.Class
# training `randomForest` to calculate feature importance
rf  <- randomForest(Class ~., data = feat)
var_imp <- varImp(rf, scale = FALSE)
# sort the score in decreasing order
var_imp_df <- data.frame(cbind(variable = rownames(var_imp), score = var_imp[,1]))
var_imp_df$score <- as.double(var_imp_df$score)
# plotting the rf_scores to determine threshold
ggplot(rf_scores, aes(x=reorder(variable, score), y=score)) +
geom_point() +
geom_segment(aes(x=variable,xend=variable,y=0,yend=score)) +
ggtitle("RF-feature plot") +
ylab("IncNodePurity") +
xlab("Variable Name") +
coord_flip()
ncortex.raw <- read.csv("https://docs.google.com/spreadsheets/d/1pWBv5zJPwyOP3vsY9AMLyuSF-ZQ-apPl/edit#gid=1571911879")
ncortex.raw <- read.csv("https://docs.google.com/spreadsheets/d/1pWBv5zJPwyOP3vsY9AMLyuSF-ZQ-apPl/edit#gid=1571911879",
header = T)
# setwd("~/Academics/gitProjects/myProjects/Mouse-protein-expr")
####### pacakges #######
packages <- c("readxl", "tidyverse", "psych", "corrplot", "randomForest",
"e1071", "caret", "neuralnet", "nnet", "NeuralSens","ipred",
"ggplot2", "FactoMineR", "factoextra", "devtools", "ggbiplot",
"pROC")
nb.cm.valid$overall[1:2]
ENS.confMat
# model-4: Stacked Ensemble Learner
evalMetrics(confMat = ENS.confMat)
evalMetrics <- function(confMat){
# metrics-1
print("The accuracy and kappa of the Naive Bayes model are")
print(round(confMat$overall[1:2] * 100,2))
# metrics-2
print("The precision, recall and F1-score for target classes are")
print(confMat$byClass[c(1,2,4,5),c(5,6,7)])
}
# model-1: Naive Bayes
evalMetrics(confMat = NB.cm.test)
# model-2: Artificial Neural Network
evalMetrics(confMat = NN.cm.test)
# model-3: Multi-nomial Logistic Regression
evalMetrics(confMat = MGR.cm.test)
# model-2: Artificial Neural Network
evalMetrics(confMat = NN.cm.test)
NN.cm.test <- NN.tablePred[2]
NN.cm.test
# model-2: Artificial Neural Network
evalMetrics(confMat = NN.cm.test)
# model-4: Stacked Ensemble Learner
evalMetrics(confMat = ENS.confMat)
stackResults[2]
source("~/Academics/gitProjects/myProjects/Mouse-protein-expr/main.R")
source("~/Academics/gitProjects/myProjects/Mouse-protein-expr/main.R")
# setwd("~/Academics/gitProjects/myProjects/Mouse-protein-expr")
####### pacakges #######
packages <- c("readxl", "tidyverse", "psych", "corrplot", "randomForest",
"e1071", "caret", "neuralnet", "nnet", "NeuralSens","ipred",
"ggplot2", "FactoMineR", "factoextra", "devtools", "ggbiplot",
"pROC")
# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
######### data review ########
# Reading data file
# define the URL -- you could dynamically build this
# URL <- "https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls"
# download file
# download.file(URL, destfile="Data_Cortex_Nuclear.xls")
# load the file
ncortex.raw <- as.data.frame(read_excel("Data_Cortex_Nuclear.xls"))
head(ncortex.raw)
### final quiz practice
cor.data <- read_excel("final-pr")
setwd("~/Academics/gitProjects/myProjects/Mouse-protein-expr")
### final quiz practice
cor.data <- read_excel("final-pr")
### final quiz practice
cor.data <- read_excel("final-pr.xlsx")
cor.data
### final quiz practice
cor.data <- data.frame(read_excel("final-pr.xlsx"))
cor.data
cor.data <- cor.data[,1:2]
corr <- cor.test(x=cor.data$x, y=cor.data$y, method = 'spearman')
cor.test(x=cor.data$x, y=cor.data$y, method = 'spearman')
cor(x=cor.data$x, y=cor.data$y, method = 'spearman')
library(xml2)
data2 <- read.csv("https://docs.google.com/spreadsheets/d/1GUlRiMRDwMxDLlRkPMchtl4kBLnBuEecoFPX5bzwxYY/edit#gid=0")
data2 <- data.frame(read_excel("final-pr.xlsx"))
data2
data2  <- data2[,1:2]
my_mod <- lm(Revenue ~ Period, data2)
my_mod
plot(data2$Period,                       # Draw line plot with regression line
data2$Revenue,
type = "l")
lines(data$x,
predict(my_mod),
col = 2,
lwd = 2)
lines(data2$Revenue,
predict(my_mod),
col = 2,
lwd = 2)
plot(data2$Period,                       # Draw line plot with regression line
data2$Revenue,
type = "l")
lines(data2$Revenue,
predict(my_mod),
col = 2,
lwd = 2)
lines(data2$Period,
predict(my_mod),
col = 2,
lwd = 2)
predic(my_mod)
predict(my_mod)
data2
lines(data2$Period[-10],
predict(my_mod),
col = 2,
lwd = 2)
plot(data2$Period,                       # Draw line plot with regression line
data2$Revenue,
type = "l")
my_mod
data2
plot(data2$Period,                       # Draw line plot with regression line
data2$Revenue,
type = "l", ylim = (0,40000))
plot(data2$Period,                       # Draw line plot with regression line
data2$Revenue,
type = "l", ylim = (c(0,40000)))
33911 + 2712
summary(my_mod)
abline(v=10, lwd=2)
predict(my_mod)
data("iris")
summary(iris)
range(iris$Sepal.Length)
data("mtcars")
summary(mtcars)
dist(mtcars)
boxplot(mtcars)
summary(mtcars)
dim(mtcars)
summary(my_mod)
RSQUARE = function(y_actual,y_predict){
cor(y_actual,y_predict)^2
}
y_predict <- predict(my_mod)
data2 <- data.frame(read_excel("final-pr.xlsx"))
y_actual <- data2$Revenue
y_act <- data2$Revenue
y_pred <- predict(my_mod)
RSQUARE(y_actual = y_act, y_predict = y_pred)
y_act <- data2$Revenue
y_act
y_act <- data2$Revenue[-10]
y_pred <- predict(my_mod)
RSQUARE(y_actual = y_act, y_predict = y_pred)
summary(my_mod)
summary(mtcars)
data("cars")
summary(cars)
summary(cars)
transformDist(c(23.2,44.3,26.7,24,5)
transformDist(c(23.2,44.3,26.7,24,5))
transformDist(c(23.2,44.3,26.7,24,5))
dataset::cars
datasets::cars
datasets::cars
cars
cars <- datasets::cars
z.scores <- cars$speed %>% scores(type = "z")
library(outliers)
install.packages('outliers')
library(outliers)
z.scores <- cars$speed %>% scores(type = "z")
z.scores
which(z.scores > 2.5)
z.scores <- cars$speed %>% scores(type = "z")
which(z.scores > 2.5)
which(cars$dist == 83)
z.scores
cars$dist == 83
cars$dist >80
tail(cars$dist)
z.scores
z.scores <- cars$dist %>% scores(type = "z")
z.scores
which(z.scores > 2.5)
cars$dist > 80
z.scores
scale(cars$dist)
which(cars$dist == 79)
cars$dist
which(cars$dist == 80)
scale(cars$dist)[23]
o <- c(34,32,223,21,"null")
o[4]
lm(formula = qsec ~ disp + wt + hp, data = mtcars)
ml1 <- lm(formula = qsec ~ disp + wt + hp, data = mtcars)
ml1.pred <- predict(ml1)
mtcars
m1 <- 89.3
confint(m1)
se1 <- 6.72
alpha = 0.05
degrees_of_freedom = sample.n - 1
aic1 <- data.frame(read_excel("final-pr.xlsx"))
aic1
aic1$seen <- as.factor(aic1$seen)
glm(seen~., data = aic1)
str(aic1)
glm(seen ~., data = aic1)
mylogit <- glm(seen ~ W + C + CW, data = aic1, family = "binomial")
summary(mylogit)
library(C50)
mod1 <- C5.0.default(Species ~., iris)
iris
str(iris)
mod1 <- C5.0.default(Species ~., iris)
mod1 <- C5.0(Species ~., iris)
confusionMatrix(mode)
confusionMatrix(mod1)
pred1 <- predict(mod1, iris, type = "class")
confusionMatrix(pred1, iris$Species)
confusionMatrix(pred1, iris$Species, mode= "prec_recall")
data2 <- read.csv("diabetes.csv")
data2
str(data2)
data2$d <- as.factor(data2$d)
lgr.mod <- glm(d~., data=data2, family = "binomial")
summary(lgr.mod)
lgr.pred <- predict(lgr.mod)
confusionMatrix(lgr.pred, data2$d)
lgr.pred
lgr.prob <- predict(lgr.mod)
lgr.pred <- ifelse(lgr.prob > 0.5, "1","0")
confusionMatrix(lgr.pred, data2$d)
lgr.pred <- as.factor(ifelse(lgr.prob > 0.5, "1","0"))
confusionMatrix(lgr.pred, data2$d)
